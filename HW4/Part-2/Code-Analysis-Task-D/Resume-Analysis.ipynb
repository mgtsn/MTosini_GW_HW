{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Analysis\n",
    "_**HARD: This is a curveball assignment. Plus, this is Python without Pandas.**_\n",
    "\n",
    "#### The objective of this assignment is for you to explain what is happening in each cell in clear, understandable language. \n",
    "\n",
    "#### _There is no need to code._ The code is there for you, and it already runs. Your task is only to explain what each line in each cell does.\n",
    "\n",
    "#### The placeholder cells should describe what happens in the cell below it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports `os` as a dependency because the `os.path.join` function. Also, the `string` dependency is needed because later in the script, `string.punctuation` will be used to detect and remove punctuation symbols. Explain what the line `from collection import Counter` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a variable that includes the path for the file we want to open. Then we create two sets that contain strings we are looking for in the file. The variables are in caps because they are constants and are not supposed to be changed in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "resume_path = os.path.join(\".\", 'resume.md')\n",
    "\n",
    "# Skills to match\n",
    "REQUIRED_SKILLS = {\"excel\", \"python\", \"mysql\", \"statistics\"}\n",
    "DESIRED_SKILLS = {\"r\", \"git\", \"html\", \"css\", \"leaflet\", \"modeling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines a function named load_file that takes one parameter named filepath. The function opens the file passed in, reads it in, changes it to lowercase, and then splits it into an list. Each element of the list is a word from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    # Helper function to read a file and return the data.\n",
    "    with open(filepath, \"r\") as resume_file_handler:\n",
    "        resume_contents = resume_file_handler.read()\n",
    "        resume_contents = resume_contents.lower()\n",
    "        resume_tokens = resume_contents.split()\n",
    "        return resume_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the function load_file wioth the resume_path variable declared earlier. The function returns a list of words from the file, which is stored in the variable 'word_list'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text for a Resume\n",
    "word_list = load_file(resume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new variable that is a set. Then, we iterate through the word list and store each element in the set. As a set's values are unique, the repeated words will still be in it only once. We print out the string 'WORDS BEFORE MOVING PUNCTUATION', using \\n to make a newline, and then print the set.Then we remove peices of punctuation that were read in as full words. We do this by getting the punctuation set from the string library, then subtracting it from the resume set. This removes any value in the punctuation set from the resume set. Then, we print 'WORDS AFTER MOVING PUNCTUATION' with another newline and print the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORDS BEFORE MOVING PUNCTUATION\n",
      "{'analytics', 'excel.', 'css,', 'd3', 'python,', 'experience', 'open-source', 'and', 'to', 'sql,', 'git/github', 'software', 'mongodb', 'from', 'hadoop,', 'tableau', 'developing', 'sets', 'api', 'statistics,', 'interactions,', 'n.', 'tables', 'performing', 'databases', 'vba', 'learning', 'intelligence', 'learning,', 'basic', 'interests', 'visualization', 'writing', 'creating', 'statistics', 'stein', 'algorithms', 'tableau,', 'media', 'boot', 'web', 'analyze', '#', 'd3,', 'skills', 'mysql', 'camp', 'hadoop', 'mining,', 'pivot', 'graduate', 'modeling', 'javascript,', 'the', 'business', 'front-end', 'excel,', 'visualizations', 'r,', 'html/css,', '*', 'files', 'machine', 'education', 'using', 'working', 'social', 'pandas', 'designing', '##', 'python', 'frank', 'scripts', 'in', 'with', 'html,', 'advanced', 'forecasting', 'data', 'mining', 'cloud', 'bootstrap,', 'contributing', 'big', 'apps', 'apis.', 'leaflet.js', 'aws', 'microsoft'}\n",
      "\n",
      "WORDS AFTER MOVING PUNCTUATION\n",
      "{'analytics', 'excel.', 'css,', 'd3', 'python,', 'experience', 'open-source', 'and', 'to', 'sql,', 'git/github', 'software', 'mongodb', 'from', 'hadoop,', 'tableau', 'developing', 'sets', 'api', 'statistics,', 'interactions,', 'n.', 'tables', 'performing', 'databases', 'vba', 'learning', 'intelligence', 'learning,', 'basic', 'interests', 'visualization', 'writing', 'creating', 'statistics', 'stein', 'algorithms', 'tableau,', 'media', 'boot', 'web', 'analyze', 'd3,', 'skills', 'mysql', 'camp', 'hadoop', 'mining,', 'pivot', 'graduate', 'modeling', 'javascript,', 'the', 'business', 'front-end', 'excel,', 'visualizations', 'r,', 'html/css,', 'files', 'machine', 'education', 'using', 'working', 'social', 'pandas', 'designing', '##', 'python', 'frank', 'in', 'scripts', 'with', 'html,', 'advanced', 'forecasting', 'data', 'mining', 'cloud', 'bootstrap,', 'contributing', 'big', 'apps', 'apis.', 'leaflet.js', 'aws', 'microsoft'}\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique words from the resume\n",
    "resume = set()\n",
    "\n",
    "# HINT: Single elements in a programming language are often referred to as tokens\n",
    "for token in word_list:\n",
    "    resume.add(token)\n",
    "\n",
    "print('\\nWORDS BEFORE MOVING PUNCTUATION')    \n",
    "print(resume)\n",
    "\n",
    "# Remove Punctuation that were read as whole words\n",
    "punctuation = set(string.punctuation)\n",
    "# HINT: Attributes that are in `resume` that are not in `punctuation` (difference)\n",
    "resume = resume - punctuation\n",
    "\n",
    "print('\\nWORDS AFTER MOVING PUNCTUATION')    \n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print 'REQUIRED SKILLS', then print the intersection of the set resume and the set REQUIRED_SKILLS. This gives us all values that are in both sets. Then we print 'DESIRED SKILLS' and the intersection between resume and DESIRED_SKILLS, doing the same thing as before with a different set. Then we use list comprehension to remove punctuation from the word list. We iterate through the list and take only the values that are not in the punctuation set (taken from string). We then print the resulting list without punctuation words. Then we remove punctuation that is part of a word. We do this by iterating through each word in the list, then each character in each of those words. We take each character that is not in the punctuation set and add it to the final list using the join function. We then print out the list after punctuation removal. Next we create a list of stop words, which are words we want to remove. Any words we want could be added to the stop words list. We then use list comprehension to iterate through the word list and add any strings not in the stop words list to the output list. This gets rid of any words from the stop words list. Finally, we print the list after removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUIRED SKILLS\n",
      "{'statistics', 'python', 'mysql'}\n",
      "DESIRED SKILLS\n",
      "{'modeling'}\n",
      "\n",
      "WORD LIST AFTER PUNCTUATION REMOVAL\n",
      "['frank', 'n.', 'stein', '##', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '##', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel.', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis.', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'front-end', 'web', 'visualizations', 'using', 'html,', 'css,', 'bootstrap,', 'd3,', 'and', 'leaflet.js', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '##', 'skills', 'microsoft', 'excel,', 'python,', 'javascript,', 'html/css,', 'api', 'interactions,', 'social', 'media', 'mining,', 'sql,', 'hadoop,', 'tableau,', 'advanced', 'statistics,', 'machine', 'learning,', 'r,', 'git/github', '##', 'interests', 'contributing', 'to', 'open-source', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html,', 'css,', 'javascript,', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER CHARACTER PUNCTUATION REMOVAL\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'using', 'html', 'css', 'bootstrap', 'd3', 'and', 'leafletjs', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'to', 'opensource', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html', 'css', 'javascript', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER STOP WORDS\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'vba', 'scripts', 'excel', 'modeling', 'forecasting', 'data', 'basic', 'statistics', 'writing', 'python', 'scripts', 'analyze', 'data', 'sets', 'from', 'files', 'apis', 'social', 'media', 'mining', 'python', 'mysql', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'html', 'css', 'bootstrap', 'd3', 'leafletjs', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'hadoop', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'opensource', 'software', 'data', 'analytics', 'python', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'html', 'css', 'javascript', 'd3', 'big', 'data', 'the', 'cloud', 'aws']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Required Skills Match using Set Intersection\n",
    "print('REQUIRED SKILLS')\n",
    "print(resume & REQUIRED_SKILLS)\n",
    "\n",
    "# Calculate the Desired Skills Match using Set Intersection\n",
    "print('DESIRED SKILLS')\n",
    "print(resume & DESIRED_SKILLS)\n",
    "\n",
    "\n",
    "# Word Punctuation Cleaning\n",
    "word_list = [word for word in word_list if word not in string.punctuation]\n",
    "print('\\nWORD LIST AFTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Character Punctuation Cleaning\n",
    "word_list = [''.join(char for char in word if char not in string.punctuation) for word in word_list]\n",
    "print('\\nWORD LIST AFTER CHARACTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Clean Stop Words\n",
    "stop_words = [\"and\", \"with\", \"using\", \"##\", \"working\", \"in\", \"to\"]\n",
    "word_list = [word for word in word_list if word not in stop_words]\n",
    "print('\\nWORD LIST AFTER STOP WORDS')\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a dictionary with keys equal to the items in word_list, and set each value to 0. We iterate over the word list and for each word we increment the value of that key in the dictionary by 1. This gives us a dictionary of how many times each word was said in the resume. Next, we count each word in the resume using the Counter function. This gives us the same result as the for loop method. We compare the two answers to show that they are equivalent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Top 10 Words\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "# Resume Word Count\n",
    "# ==========================\n",
    "# Initialize a dictionary with default values equal to zero\n",
    "word_count = {}.fromkeys(word_list, 0)\n",
    "\n",
    "# Loop through the word list and count each word.\n",
    "for word in word_list:\n",
    "    word_count[word] += 1\n",
    "# print(word_count)\n",
    "\n",
    "# Bonus using collections.Counter\n",
    "word_counter = Counter(word_list)\n",
    "# print(word_counter)\n",
    "\n",
    "# Comparing both word count solutions\n",
    "print(word_count == word_counter)\n",
    "\n",
    "# Top 10 Words\n",
    "print(\"Top 10 Words\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with your clear explanation of what happens in the cell below. Which column was sorted and how? How was the top ten selected? Does that explain the significance of `[:10]`?\n",
    "\n",
    "As a bonus, explain how you would get rid of the blank token.\n",
    "We sort the dictionary using the sorted() method. As paramaters we pass in the dictionary, the values, and reverse=True. This tell us to sort the dictionary by the values, and to sort them in descending order. We loop through the values in the sorted dictionary using a for loop. We look at only the first 10 values due to the [:10]. This tells us to start at index 0 and go up to but not including index 10. For each word, we print out an fstring of the format 'Token: {token name} Count: {word count}'. In order to get rid of the space, you could use list comprehension to iterate over the list and return only the values that are not null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: data                 Count: 7\n",
      "Token:                      Count: 4\n",
      "Token: python               Count: 4\n",
      "Token: analytics            Count: 3\n",
      "Token: visualization        Count: 2\n",
      "Token: scripts              Count: 2\n",
      "Token: excel                Count: 2\n",
      "Token: statistics           Count: 2\n",
      "Token: social               Count: 2\n",
      "Token: media                Count: 2\n"
     ]
    }
   ],
   "source": [
    "# Sort words by count and print the top 10\n",
    "sorted_words = []\n",
    "for word in sorted(word_count, key=word_count.get, reverse=True)[:10]:\n",
    "    print(f\"Token: {word:20} Count: {word_count[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
